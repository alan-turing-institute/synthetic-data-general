# Synthetic Data Seminar Series 11/11/12 1-2pm

_Callum's Notes and thoughts off the back of Lukasz's introductory talk and some subsequent discussion with Camila_.

There is a lot of hype about synthetic data. Synthetic data refers to any data that has been generated using a model, which in this business are called _synthetic data generators_. Synthetic data generators are by design a function of the raw data.

Synthetic data has ambitious promises. The domain boils down to (vaguely) making a fake dataset that resembles the raw dataset in key ways, yet is also different from the raw dataset in similarly important ways. Perhaps we can use synthetic data for:
- sharing sensitive data in a privacy preserving way
- accelerating the development of machine learning projects
- mitigating biases in data (e.g. historical biases)

For the most part, the issues of synthetic data are shared with most statistical modelling problems. We first assume that the data is generated by an unkown probabilistic process, then try to build a model that mimics the process (though model interpretability is not deemed that important). The resulting data output needs to be plausible for all potential inputs to the model (_syntactical accuracy_), but also preserve key statistical propertie of the original data (_statistical accuracy_). 

There are, however, key ways in which the requirements of synthetic data (or more precisely, synthetic data _generators_) seem to differ from statistical modelling. Foremost of these is that a notion of _privacy_ can be quantified. Privacy is often understood in terms of information leakage (for example, what information is leaked by your inclusion/exclusion in the raw dataset). Quantifying information leakage is often approached using information theory. Next is _utility_: a synthetic dataset needs to be useful for the purpose for which it is generated. Utility can only ever be evaluated in context (e.g. can we replicate the same set of inferential conclusions from the raw data with synthetic data that has privacy gaurantees?). This represents a trade-off: a synthetic dataset _may_ (but not always) be more useful if it is more similar to the raw dataset, but a synthetic dataset is undoubtedly more private the more dissimilar it is to the raw data.

Considering the trade-off brings a third notion - of dataset _fidelity_. Fidelity considers how statistically accurate a synthetic data needs to be (or in what ways it needs to be statistically accurate) to have a sufficiently high utility. **Privacy**, **Utility**, and **Fidelity** are referred to as the three pillars of synthetic data.

It seems to me (Callum) that any synthetic data problem is only tangible when one has a use-case and subsequently a way to measure utility. Then one can set a utility threshold and consequently assess whether fidelity can be minimised (made dissimilar) enough to achieve an acceptable level of privacy (with respect to the utility threshold). The answer may be that it cannot. 

What is an acceptable level of privacy also seems to me to be a moving target for synthetic data problems. There are methods of statistically providing privacy guarantees. For example with differential privacy the utility/privacy trade-off is controlled by the level of noise added to the synthetic data during generation (noise can be added at different levels, for example at the data point level or the parameter level). The more noise added, the less the inclusion of your individual data affects the overall dataset. But the _acceptable_ level of privacy might differ per organisation, since the benefits of releasing data with some privacy risk might be greater than releasing no data at all.

A final thing to note is that synthetic data researchers (and potential users) seem to be mainly interested in datasets with high dimensions (many variables). This means that the field has gravitated towards black-box machine learning models that scale well and supposedly (I'm no expert) provide good performance in high dimensions (e.g. GANs are one example).  

In the latter section of the talk Lukasz talks in more detail about what you may use synthetic data for, and why. In general, the problems seem to share a commonality where lots of variations of the existing dataset would be useful, but actually collecting those variations would be costly. For example, providing labelled data for deep neural networks, or increasing robustness (generality; combatting overfitting) of machine learning models by throwing many variations of data at at it (analogous to leave-one-out cross validation). Lukasz also discussed the possibility of combating historical biases in datasets with reference to Tiwald et al., 2021. I didn't follow the logic here. It seems that the historical bias would need to be well known and the model to be sufficiently interpretable to include a counter to the bias. I suppose I should read the paper.

Some key slides from the talk:


![](https://i.imgur.com/3DTSDqb.png)

![](https://i.imgur.com/yXXdiAl.png)

![](https://i.imgur.com/sqzLqCu.png)

![](https://i.imgur.com/9MDENwi.png)

